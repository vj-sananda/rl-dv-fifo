{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Q-Network (DQN)\n",
    "---\n",
    "In this notebook, you will implement a DQN agent with OpenAI Gym's LunarLander-v2 environment.\n",
    "\n",
    "### 1. Import the Necessary Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Instantiate the Environment and Agent\n",
    "\n",
    "Initialize the environment in the code cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "State shape:  (8,)\n",
      "Number of actions:  4\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "env.seed(0)\n",
    "print('State shape: ', env.observation_space.shape)\n",
    "print('Number of actions: ', env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heuristic(env, s):\n",
    "    \"\"\"\n",
    "    The heuristic for\n",
    "    1. Testing\n",
    "    2. Demonstration rollout.\n",
    "\n",
    "    Args:\n",
    "        env: The environment\n",
    "        s (list): The state. Attributes:\n",
    "                  s[0] is the horizontal coordinate\n",
    "                  s[1] is the vertical coordinate\n",
    "                  s[2] is the horizontal speed\n",
    "                  s[3] is the vertical speed\n",
    "                  s[4] is the angle\n",
    "                  s[5] is the angular speed\n",
    "                  s[6] 1 if first leg has contact, else 0\n",
    "                  s[7] 1 if second leg has contact, else 0\n",
    "    returns:\n",
    "         a: The heuristic to be fed into the step function defined above to determine the next step and reward.\n",
    "    \"\"\"\n",
    "\n",
    "    angle_targ = s[0]*0.5 + s[2]*1.0         # angle should point towards center\n",
    "    if angle_targ > 0.4: angle_targ = 0.4    # more than 0.4 radians (22 degrees) is bad\n",
    "    if angle_targ < -0.4: angle_targ = -0.4\n",
    "    hover_targ = 0.55*np.abs(s[0])           # target y should be proportional to horizontal offset\n",
    "\n",
    "    angle_todo = (angle_targ - s[4]) * 0.5 - (s[5])*1.0\n",
    "    hover_todo = (hover_targ - s[1])*0.5 - (s[3])*0.5\n",
    "\n",
    "    if s[6] or s[7]:  # legs have contact\n",
    "        angle_todo = 0\n",
    "        hover_todo = -(s[3])*0.5  # override to reduce fall speed, that's all we need after contact\n",
    "\n",
    "    if env.continuous:\n",
    "        a = np.array([hover_todo*20 - 1, -angle_todo*20])\n",
    "        a = np.clip(a, -1, +1)\n",
    "    else:\n",
    "        a = 0\n",
    "        if hover_todo > np.abs(angle_todo) and hover_todo > 0.05: a = 2\n",
    "        elif angle_todo < -0.05: a = 3\n",
    "        elif angle_todo > +0.05: a = 1\n",
    "    return a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Please refer to the instructions in `Deep_Q_Network.ipynb` if you would like to write your own DQN agent.  Otherwise, run the code cell below to load the solution files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward = -100\n",
      "Reward = -100\n",
      "Reward = -100\n",
      "Reward = -100\n",
      "Reward = -100\n",
      "Reward = -100\n",
      "Reward = -100\n",
      "Reward = -100\n",
      "Reward = -100\n",
      "Reward = -100\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-37-9263eb6db88b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0;31m#action = heuristic(env,state)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m         \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;31m#print(\"action:{} state:{}, reward:{}, done:{}\".format(action,state,reward,done))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python3/lib/python3.6/site-packages/gym/core.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode, **kwargs)\u001b[0m\n\u001b[1;32m    238\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    239\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'human'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 240\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    241\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python3/lib/python3.6/site-packages/gym/envs/box2d/lunar_lander.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, mode)\u001b[0m\n\u001b[1;32m    374\u001b[0m                                      color=(0.8, 0.8, 0))\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 376\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mviewer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrender\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreturn_rgb_array\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'rgb_array'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    377\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    378\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python3/lib/python3.6/site-packages/gym/envs/classic_control/rendering.py\u001b[0m in \u001b[0;36mrender\u001b[0;34m(self, return_rgb_array)\u001b[0m\n\u001b[1;32m    125\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mheight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbuffer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwidth\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m             \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0marr\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 127\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    128\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0monetime_geoms\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0marr\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_rgb_array\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misopen\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python3/lib/python3.6/site-packages/pyglet/window/cocoa/__init__.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    287\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdraw_mouse_cursor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdispatch_events\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python3/lib/python3.6/site-packages/pyglet/gl/cocoa.py\u001b[0m in \u001b[0;36mflip\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    325\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mflip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 327\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nscontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflushBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda2/envs/python3/lib/python3.6/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    783\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m         \u001b[0;34m\"\"\"Call the method with the given arguments.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobjc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m \u001b[0;31m######################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/python3/lib/python3.6/site-packages/pyglet/libs/darwin/cocoapy/runtime.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, objc_id, *args)\u001b[0m\n\u001b[1;32m    753\u001b[0m         \u001b[0mf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_callable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    754\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 755\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobjc_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselector\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    756\u001b[0m             \u001b[0;31m# Convert result to python type if it is a instance or class pointer.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    757\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrestype\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mObjCInstance\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from dqn_agent import Agent\n",
    "\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "\n",
    "for i in range(30):\n",
    "    # watch an untrained agent\n",
    "    state = env.reset()\n",
    "    for j in range(200):\n",
    "        #action = agent.act(state)\n",
    "        #action = agent.act(state)\n",
    "        #action = heuristic(env,state)\n",
    "        action = np.random.choice(np.arange(env.action_space.n))\n",
    "        env.render()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        #print(\"action:{} state:{}, reward:{}, done:{}\".format(action,state,reward,done))\n",
    "        if done:\n",
    "            print(\"Reward = {}\".format(reward))\n",
    "            break \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dqn_agent import Agent\n",
    "\n",
    "agent = Agent(state_size=8, action_size=4, seed=0)\n",
    "\n",
    "for i in range(30):\n",
    "    # watch an untrained agent\n",
    "    state = env.reset()\n",
    "    for j in range(200):\n",
    "        #action = agent.act(state)\n",
    "        #action = agent.act(state)\n",
    "        #action = heuristic(env,state)\n",
    "        action = np.random.choice(np.arange(env.action_space.n))\n",
    "        env.render()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        #print(\"action:{} state:{}, reward:{}, done:{}\".format(action,state,reward,done))\n",
    "        if done:\n",
    "            print(\"Reward = {}\".format(reward))\n",
    "            break \n",
    "        \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Train the Agent with DQN\n",
    "\n",
    "Run the code cell below to train the agent from scratch.  You are welcome to amend the supplied values of the parameters in the function, to try to see if you can get better performance!\n",
    "\n",
    "Alternatively, you can skip to the next step below (**4. Watch a Smart Agent!**), to load the saved model weights from a pre-trained agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> <ipython-input-38-cc5236d6b99b>(20)dqn()\n",
      "-> action = agent.act(state, eps)\n",
      "(Pdb) state\n",
      "array([ 0.0017664 ,  1.4136981 ,  0.17891248,  0.12346578, -0.00204014,\n",
      "       -0.04052637,  0.        ,  0.        ], dtype=float32)\n",
      "(Pdb) type(state)\n",
      "<class 'numpy.ndarray'>\n",
      "(Pdb) action\n",
      "1\n",
      "(Pdb) type(action)\n",
      "<class 'numpy.int64'>\n",
      "(Pdb) s\n",
      "--Call--\n",
      "> /Users/vsananda/learning/udacity/reinforcement-learning-nano/deep-reinforcement-learning/dqn/solution/dqn_agent.py(58)act()\n",
      "-> def act(self, state, eps=0.):\n",
      "(Pdb) s\n",
      "> /Users/vsananda/learning/udacity/reinforcement-learning-nano/deep-reinforcement-learning/dqn/solution/dqn_agent.py(66)act()\n",
      "-> state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
      "(Pdb) s\n",
      "> /Users/vsananda/learning/udacity/reinforcement-learning-nano/deep-reinforcement-learning/dqn/solution/dqn_agent.py(67)act()\n",
      "-> self.qnetwork_local.eval()\n",
      "(Pdb) s\n",
      "--Call--\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(1075)eval()\n",
      "-> def eval(self):\n",
      "(Pdb) s\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(1088)eval()\n",
      "-> return self.train(False)\n",
      "(Pdb) s\n",
      "--Call--\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(1055)train()\n",
      "-> def train(self, mode=True):\n",
      "(Pdb) s\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(1070)train()\n",
      "-> self.training = mode\n",
      "(Pdb) s\n",
      "--Call--\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(596)__setattr__()\n",
      "-> def __setattr__(self, name, value):\n",
      "(Pdb) s\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(597)__setattr__()\n",
      "-> def remove_from(*dicts):\n",
      "(Pdb) s\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(602)__setattr__()\n",
      "-> params = self.__dict__.get('_parameters')\n",
      "(Pdb) s\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(603)__setattr__()\n",
      "-> if isinstance(value, Parameter):\n",
      "(Pdb) s\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(609)__setattr__()\n",
      "-> elif params is not None and name in params:\n",
      "(Pdb) s\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(616)__setattr__()\n",
      "-> modules = self.__dict__.get('_modules')\n",
      "(Pdb) n\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(617)__setattr__()\n",
      "-> if isinstance(value, Module):\n",
      "(Pdb) s\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(623)__setattr__()\n",
      "-> elif modules is not None and name in modules:\n",
      "(Pdb) n\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(630)__setattr__()\n",
      "-> buffers = self.__dict__.get('_buffers')\n",
      "(Pdb) n\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(631)__setattr__()\n",
      "-> if buffers is not None and name in buffers:\n",
      "(Pdb) n\n",
      "> /Users/vsananda/anaconda2/envs/python3/lib/python3.6/site-packages/torch/nn/modules/module.py(638)__setattr__()\n",
      "-> object.__setattr__(self, name, value)\n",
      "(Pdb) h\n",
      "\n",
      "Documented commands (type help <topic>):\n",
      "========================================\n",
      "EOF    c          d        h         list      q        rv       undisplay\n",
      "a      cl         debug    help      ll        quit     s        unt      \n",
      "alias  clear      disable  ignore    longlist  r        source   until    \n",
      "args   commands   display  interact  n         restart  step     up       \n",
      "b      condition  down     j         next      return   tbreak   w        \n",
      "break  cont       enable   jump      p         retval   u        whatis   \n",
      "bt     continue   exit     l         pp        run      unalias  where    \n",
      "\n",
      "Miscellaneous help topics:\n",
      "==========================\n",
      "exec  pdb\n",
      "\n",
      "(Pdb) help n\n",
      "n(ext)\n",
      "        Continue execution until the next line in the current function\n",
      "        is reached or it returns.\n",
      "(Pdb) help s\n",
      "s(tep)\n",
      "        Execute the current line, stop at the first possible occasion\n",
      "        (either in a function that is called or in the current\n",
      "        function).\n",
      "(Pdb) help so\n",
      "*** No help for 'so'\n",
      "(Pdb) helpc\n",
      "*** NameError: name 'helpc' is not defined\n",
      "(Pdb) help c\n",
      "c(ont(inue))\n",
      "        Continue execution, only stop when a breakpoint is encountered.\n",
      "(Pdb) help breakpoint\n",
      "*** No help for 'breakpoint'\n",
      "(Pdb) h\n",
      "\n",
      "Documented commands (type help <topic>):\n",
      "========================================\n",
      "EOF    c          d        h         list      q        rv       undisplay\n",
      "a      cl         debug    help      ll        quit     s        unt      \n",
      "alias  clear      disable  ignore    longlist  r        source   until    \n",
      "args   commands   display  interact  n         restart  step     up       \n",
      "b      condition  down     j         next      return   tbreak   w        \n",
      "break  cont       enable   jump      p         retval   u        whatis   \n",
      "bt     continue   exit     l         pp        run      unalias  where    \n",
      "\n",
      "Miscellaneous help topics:\n",
      "==========================\n",
      "exec  pdb\n",
      "\n",
      "(Pdb) help b\n",
      "b(reak) [ ([filename:]lineno | function) [, condition] ]\n",
      "        Without argument, list all breaks.\n",
      "\n",
      "        With a line number argument, set a break at this line in the\n",
      "        current file.  With a function name, set a break at the first\n",
      "        executable line of that function.  If a second argument is\n",
      "        present, it is a string specifying an expression which must\n",
      "        evaluate to true before the breakpoint is honored.\n",
      "\n",
      "        The line number may be prefixed with a filename and a colon,\n",
      "        to specify a breakpoint in another file (probably one that\n",
      "        hasn't been loaded yet).  The file is searched for on\n",
      "        sys.path; the .py suffix may be omitted.\n",
      "(Pdb) \n",
      "b(reak) [ ([filename:]lineno | function) [, condition] ]\n",
      "        Without argument, list all breaks.\n",
      "\n",
      "        With a line number argument, set a break at this line in the\n",
      "        current file.  With a function name, set a break at the first\n",
      "        executable line of that function.  If a second argument is\n",
      "        present, it is a string specifying an expression which must\n",
      "        evaluate to true before the breakpoint is honored.\n",
      "\n",
      "        The line number may be prefixed with a filename and a colon,\n",
      "        to specify a breakpoint in another file (probably one that\n",
      "        hasn't been loaded yet).  The file is searched for on\n",
      "        sys.path; the .py suffix may be omitted.\n",
      "(Pdb) \n",
      "b(reak) [ ([filename:]lineno | function) [, condition] ]\n",
      "        Without argument, list all breaks.\n",
      "\n",
      "        With a line number argument, set a break at this line in the\n",
      "        current file.  With a function name, set a break at the first\n",
      "        executable line of that function.  If a second argument is\n",
      "        present, it is a string specifying an expression which must\n",
      "        evaluate to true before the breakpoint is honored.\n",
      "\n",
      "        The line number may be prefixed with a filename and a colon,\n",
      "        to specify a breakpoint in another file (probably one that\n",
      "        hasn't been loaded yet).  The file is searched for on\n",
      "        sys.path; the .py suffix may be omitted.\n",
      "(Pdb) c\n",
      "> <ipython-input-38-cc5236d6b99b>(19)dqn()\n",
      "-> pdb.set_trace()\n",
      "(Pdb) c\n",
      "> <ipython-input-38-cc5236d6b99b>(20)dqn()\n",
      "-> action = agent.act(state, eps)\n"
     ]
    }
   ],
   "source": [
    "def dqn(n_episodes=2000, max_t=1000, eps_start=1.0, eps_end=0.01, eps_decay=0.995):\n",
    "    \"\"\"Deep Q-Learning.\n",
    "    \n",
    "    Params\n",
    "    ======\n",
    "        n_episodes (int): maximum number of training episodes\n",
    "        max_t (int): maximum number of timesteps per episode\n",
    "        eps_start (float): starting value of epsilon, for epsilon-greedy action selection\n",
    "        eps_end (float): minimum value of epsilon\n",
    "        eps_decay (float): multiplicative factor (per episode) for decreasing epsilon\n",
    "    \"\"\"\n",
    "    scores = []                        # list containing scores from each episode\n",
    "    scores_window = deque(maxlen=100)  # last 100 scores\n",
    "    eps = eps_start                    # initialize epsilon\n",
    "    for i_episode in range(1, n_episodes+1):\n",
    "        state = env.reset()\n",
    "        score = 0\n",
    "        for t in range(max_t):\n",
    "            pdb.set_trace()\n",
    "            action = agent.act(state, eps)\n",
    "            next_state, reward, done, _ = env.step(action)\n",
    "            agent.step(state, action, reward, next_state, done)\n",
    "            state = next_state\n",
    "            score += reward\n",
    "            if done:\n",
    "                break \n",
    "        scores_window.append(score)       # save most recent score\n",
    "        scores.append(score)              # save most recent score\n",
    "        eps = max(eps_end, eps_decay*eps) # decrease epsilon\n",
    "        print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)), end=\"\")\n",
    "        if i_episode % 100 == 0:\n",
    "            print('\\rEpisode {}\\tAverage Score: {:.2f}'.format(i_episode, np.mean(scores_window)))\n",
    "        if np.mean(scores_window)>=200.0:\n",
    "            print('\\nEnvironment solved in {:d} episodes!\\tAverage Score: {:.2f}'.format(i_episode-100, np.mean(scores_window)))\n",
    "            torch.save(agent.qnetwork_local.state_dict(), 'checkpoint.pth')\n",
    "            break\n",
    "    return scores\n",
    "\n",
    "scores = dqn()\n",
    "\n",
    "# plot the scores\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(scores)), scores)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Watch a Smart Agent!\n",
    "\n",
    "In the next code cell, you will load the trained weights from file to watch a smart agent!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reward = -2.2894592989350557\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = 0.19197308410352037\n",
      "Reward = 100\n",
      "Reward = 0.08566658476196423\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = -100\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = -0.006392354695715169\n",
      "Reward = 100\n",
      "Reward = -0.017451523130886243\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = 100\n",
      "Reward = -0.5311311990440999\n",
      "Reward = 100\n"
     ]
    }
   ],
   "source": [
    "# load the weights from file\n",
    "agent.qnetwork_local.load_state_dict(torch.load('checkpoint.pth'))\n",
    "\n",
    "for i in range(30):\n",
    "    state = env.reset()\n",
    "    for j in range(2000):\n",
    "        action = agent.act(state)\n",
    "        env.render()\n",
    "        state, reward, done, _ = env.step(action)\n",
    "        if done:\n",
    "            print(\"Reward = {}\".format(reward))\n",
    "            break \n",
    "            \n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Explore\n",
    "\n",
    "In this exercise, you have implemented a DQN agent and demonstrated how to use it to solve an OpenAI Gym environment.  To continue your learning, you are encouraged to complete any (or all!) of the following tasks:\n",
    "- Amend the various hyperparameters and network architecture to see if you can get your agent to solve the environment faster.  Once you build intuition for the hyperparameters that work well with this environment, try solving a different OpenAI Gym task with discrete actions!\n",
    "- You may like to implement some improvements such as prioritized experience replay, Double DQN, or Dueling DQN! \n",
    "- Write a blog post explaining the intuition behind the DQN algorithm and demonstrating how to use it to solve an RL environment of your choosing.  "
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
